{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Explore here"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Paso 1: Planteamiento del problema y recopilación de datos\n",
                "\n",
                "\n",
                "**Prediciendo la diabetes**\n",
                "\n",
                "Este conjunto de datos proviene originalmente del Instituto Nacional de Diabetes y Enfermedades Digestivas y Renales. El objetivo es predecir en base a medidas diagnósticas si un paciente tiene o no diabetes.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mRunning cells with 'Python 3.12.12' requires the ipykernel package.\n",
                        "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import math\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "import json\n",
                "from numpy._core.defchararray import upper\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
                "import pickle\n",
                "from sklearn.feature_selection import f_classif, SelectKBest\n",
                "from sklearn.feature_selection import SelectKBest\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import accuracy_score\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "url = \"https://breathecode.herokuapp.com/asset/internal-link?id=930&path=diabetes.csv\"\n",
                "total_data = pd.read_csv(url)\n",
                "total_data.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Paso 2: Exploración y limpieza de datos\n",
                "Hay que conocer las dimensiones y tipologías de datos del objeto con el que estamos trabajando es vital. Para ello necesitamos el atributo shape para obtener las dimensiones del objeto y la función info() para conocer la tipología y la cantidad de valores no nulos:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Conocer las dimensiones\n",
                "total_data.shape\n",
                "print(f\"Hay {total_data.shape[0]} filas y {total_data.shape[1]} columnas\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Obtener información sobre tipos de datos y valores no nulos\n",
                "total_data.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vars_numericas = total_data.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
                "vars_categoricas = total_data.select_dtypes(include=[\"object\"]).columns.tolist()\n",
                "print(vars_categoricas,vars_numericas)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "No hay variables categóricas\n",
                "\n",
                "\n",
                "**Eliminación de duplicados**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "num_duplicates = total_data.duplicated().sum()\n",
                "print(f\"Número de filas duplicadas: {num_duplicates}\")\n",
                "\n",
                "if num_duplicates > 0:\n",
                "    total_data = total_data.drop_duplicates().reset_index(drop = True)\n",
                "    print(f\"Número duplicados borrados. Ahora hay {total_data.shape[0]} datos\")\n",
                "else:\n",
                "    print(\"No se han encontrado duplicados\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Paso 3: Análisis de variables univariante\n",
                "Una variable univariante es un término estadístico que se emplea para referenciar un conjunto de observaciones de un atributo. Esto es, el análisis columna a columna del DataFrame. Para ello, debemos distinguir si una variable es categórica o numérica, ya que el cuerpo del análisis y las conclusiones que se pueden obtener serán distintas.\n",
                "\n",
                "**Análisis sobre variables numéricas**:\n",
                "\n",
                "Una variable numérica es un tipo de variable que puede tomar valores numéricos (enteros, fracciones, decimales, negativos, etc.) en un rango infinito. Una variable categórica numérica puede ser también una variable numérica (por ejemplo, para los sucesivos análisis, podemos tomar la clase Survived como numérica también para estudiar relaciones). Normalmente se representan utilizando un histograma y diagramas de caja, expuestos juntos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Las variables numéricas son {vars_numericas}\")\n",
                "\n",
                "n_cols = 4\n",
                "n_rows = 2*(math.ceil(len(vars_numericas) / n_cols))\n",
                "\n",
                "fig, axis = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
                "axis = axis.flatten()\n",
                "\n",
                "for i, col in enumerate(vars_numericas):\n",
                "    sns.histplot(data=total_data, x=col, ax=axis[2*i])\n",
                "    axis[2*i].set_title(f\"Histograma {col}\")\n",
                "\n",
                "    sns.boxplot(data=total_data, x=col, ax=axis[2*i + 1])\n",
                "    axis[2*i + 1].set_title(f\"Boxplot {col}\")\n",
                "\n",
                "# Eliminar subplots sobrantes\n",
                "for j in range(2*i + 2, len(axis)):\n",
                "    fig.delaxes(axis[j])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Paso 4: Análisis de variables multivariante\n",
                "\n",
                "Tras analizar las características una a una, es momento de analizarlas en relación con la predictora y con ellas mismas, para sacar conclusiones más claras acerca de sus relaciones y poder tomar decisiones sobre su procesamiento.\n",
                "\n",
                "Así, si quisiéramos eliminar una variable debido a una alta cantidad de valores nulos o ciertos outliers, es necesario antes aplicar este proceso para asegurar que la eliminación de ciertos valores no son críticos\n",
                "\n",
                "**Análisis numérico-numérico:**\n",
                "\n",
                "Cuando las dos variables que se comparan tienen datos numéricos, se dice que el análisis es numérico-numérico. Para comparar dos columnas numéricas se utilizan diagramas de dispersión y análisis de correlaciones."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "n_cols = 4\n",
                "target=\"Outcome\"\n",
                "vars_num_sin_pred=[col for col in vars_numericas if col != target]\n",
                "n_rows =2*( math.ceil(len(vars_num_sin_pred) / n_cols))\n",
                "\n",
                "fig, axis = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
                "axis = axis.flatten()\n",
                "for i, col in enumerate(vars_num_sin_pred):\n",
                "    sns.regplot(ax = axis[2*i], data = total_data, x = col, y = target,line_kws={\"color\": \"red\"})\n",
                "    axis[2*i].set_title(f\"Regplot {col}\")\n",
                "    sns.heatmap(total_data[[target,col]].corr(), annot = True, fmt = \".2f\", ax = axis[2*i + 1], cbar = False)\n",
                "    axis[2*i + 1].set_title(f\"Heatmap {col}\")\n",
                "\n",
                "# Eliminar subplots sobrantes\n",
                "for j in range(2*i + 2, len(axis)):\n",
                "    fig.delaxes(axis[j])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vars_final=vars_categoricas+vars_numericas\n",
                "total_data=total_data[vars_final]\n",
                "fig, ax = plt.subplots(figsize=(15,15))\n",
                "sns.heatmap(total_data[vars_final].corr(method=\"pearson\"), annot=True, fmt=\".2f\", cmap=\"viridis\", ax=ax)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Las mas correlacionadas son la glucosa, el BMI y la cantidad de embarazos. Las otras variables tienen correlaciones menores a 0.20"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Paso 5: Ingeniería de características\n",
                "\n",
                "La ingeniería de características (*feature engineering*) es un proceso que implica la creación de nuevas características (o variables) a partir de las existentes para mejorar el rendimiento del modelo. Esto puede implicar una variedad de técnicas como la normalización, la transformación de los datos, etcétera. El objetivo es mejorar la precisión del modelo y/o reducir la complejidad del mismo, facilitando así su interpretación.\n",
                "\n",
                "En los pasos previos hemos empezado a trabajar con los datos eliminando duplicados, contabilizando los valores nulos e, incluso, para calcular correlaciones. Si bien esto podríamos haberlo hecho en este paso, ya que forma parte de la ingeniería de características, normalmente suele hacerse antes de analizar las variables, separando este proceso en uno previo y este que vamos a ver a continuación.\n",
                "\n",
                "**Análisis de outliers**\n",
                "\n",
                "Un valor atípico (*outlier*) es un punto de datos que se desvía significativamente de los demás. Es un valor que es notablemente diferente de lo que sería de esperar dada la tendencia general de los datos. Estos outliers pueden ser causados por errores en la recolección de datos, variaciones naturales en los datos, o pueden ser indicativos de algo significativo, como una anomalía o evento extraordinario.\n",
                "\n",
                "El análisis descriptivo es una poderosa herramienta para caracterizar el conjunto de datos: la media, desviación y los cuartiles nos brindan una poderosa información sobre cada variable. La función `describe()` de un DataFrame nos ayuda a calcular en unos tiempos muy reducidos todos estos valores."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "n_cols = 4\n",
                "n_rows = math.ceil(len(vars_final) / n_cols)\n",
                "\n",
                "fig, axis = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
                "axis = axis.flatten()\n",
                "\n",
                "for i, col in enumerate(vars_final):\n",
                "    sns.boxplot(data=total_data, x=col, ax=axis[i])\n",
                "    axis[i].set_title(f\"Boxplot {col}\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "total_data.describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Todas las variables sufren de datos atipicos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "predictoras=[col for col in vars_final if col != target]\n",
                "total_data_CON_outliers = total_data.copy()\n",
                "total_data_SIN_outliers = total_data.copy() # Lo vamos a modificar. Para cada variable con outliers, reemplazamos sus valores outliers\n",
                "\n",
                "outliers_cols = predictoras\n",
                "\n",
                "def replace_outliers(column, df):\n",
                "  col_stats = total_data[column].describe()\n",
                "  col_iqr = col_stats[\"75%\"] - col_stats[\"25%\"]\n",
                "  upper_limit = round(float(col_stats[\"75%\"] + 1.5 * col_iqr), 2)\n",
                "  lower_limit = round(float(col_stats[\"25%\"] - 1.5 * col_iqr), 2)\n",
                "\n",
                "  if lower_limit < 0: lower_limit = min(df[column])\n",
                "  # Vamos a quitar los outliers superiores\n",
                "  df[column] = df[column].apply(lambda x: x if (x <= upper_limit) else upper_limit)\n",
                "  # Vamos a quitar los outliers inferiores\n",
                "  df[column] = df[column].apply(lambda x: x if (x >= lower_limit) else lower_limit)\n",
                "  return df.copy(), [lower_limit, upper_limit]\n",
                "\n",
                "outliers_dict = {}\n",
                "for column in outliers_cols:\n",
                "  total_data_SIN_outliers, limits = replace_outliers(column, total_data_SIN_outliers)\n",
                "  outliers_dict.update({column: limits})\n",
                "\n",
                "outliers_dict # Este JSON me lo tengo que GUARDAR\n",
                "with open(\"./outliers_dict.json\", \"w\") as f:\n",
                "  json.dump(outliers_dict, f)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Análisis de valores faltantes**\n",
                "\n",
                "Un *valor faltante* (*missing value*) es un espacio que no tiene valor asignado en la observación de una variable específica. Este tipo de valores son bastante comunes y pueden surgir por muchas razones. Por ejemplo, podría haber un error en la recopilación de datos, alguien podría haberse negado a responder una pregunta en una encuesta, o simplemente podría ser que cierta información no esté disponible o no sea aplicable.\n",
                "\n",
                "La función `isnull()` es una poderosa herramienta para obtener esta información:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "total_data_CON_outliers.isnull().sum().sort_values(ascending=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "No hay nulos en ninguna de los data sets\n",
                "\n",
                "## Paso 5.5: Aplicación\n",
                "\n",
                "El *escalado de valores* (*feature scaling*) es un paso crucial en el preprocesamiento de datos para muchos algoritmos de Machine Learning. Es una técnica que cambia el rango de los valores de los datos para que puedan ser comparables entre sí. El escalado normalmente implica la normalización, que es el proceso de cambiar los valores para que tengan una media de 0 y una desviación estándar de 1. Otra técnica común es el escalado mínimo-máximo, que transforma los datos para que todos los valores estén entre 0 y 1.\n",
                "\n",
                "Antes de escalar los valores, debemos asegurarnos de que todas nuestras variables predictoras sean numéricas. Una vez preparadas, podemos aplicar técnicas como la normalización o el escalado Min-Max directamente sobre el dataset completo. Más adelante, cuando pasemos a la fase de modelado, será el momento de dividir los datos en entrenamiento y prueba.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# total_data_SIN_outliers\n",
                "# total_data_CON_outliers\n",
                "X_CON = total_data_CON_outliers.drop(target, axis = 1)[predictoras]\n",
                "X_SIN = total_data_SIN_outliers.drop(target, axis = 1)[predictoras]\n",
                "y = total_data_CON_outliers[target]\n",
                "\n",
                "X_train_CON_outliers, X_test_CON_outliers, y_train, y_test = train_test_split(X_CON, y, test_size = 0.2, random_state = 10)\n",
                "X_train_SIN_outliers, X_test_SIN_outliers = train_test_split(X_SIN, test_size = 0.2, random_state = 10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# X_train_CON_outliers\n",
                "# X_train_SIN_outliers\n",
                "\n",
                "# X_test_CON_outliers\n",
                "# X_test_SIN_outliers\n",
                "\n",
                "# TENGO QUE GUARDARME TRES SITUACIONES: 1) DATASET SIN TOCAR, 2) DATASET NORMALIZADO, 3) DATASET MIN-MAX\n",
                "\n",
                "## NORMALIZACIÓN\n",
                "### CON OUTLIERS\n",
                "norm_CON_outliers = StandardScaler()\n",
                "norm_CON_outliers.fit(X_train_CON_outliers)\n",
                "\n",
                "X_train_CON_outliers_norm = norm_CON_outliers.transform(X_train_CON_outliers)\n",
                "X_train_CON_outliers_norm = pd.DataFrame(X_train_CON_outliers_norm, index = X_train_CON_outliers.index, columns = predictoras)\n",
                "\n",
                "X_test_CON_outliers_norm = norm_CON_outliers.transform(X_test_CON_outliers)\n",
                "X_test_CON_outliers_norm = pd.DataFrame(X_test_CON_outliers_norm, index = X_test_CON_outliers.index, columns = predictoras)\n",
                "\n",
                "### SIN OUTLIERS\n",
                "norm_SIN_outliers = StandardScaler()\n",
                "norm_SIN_outliers.fit(X_train_SIN_outliers)\n",
                "\n",
                "X_train_SIN_outliers_norm = norm_SIN_outliers.transform(X_train_SIN_outliers)\n",
                "X_train_SIN_outliers_norm = pd.DataFrame(X_train_SIN_outliers_norm, index = X_train_SIN_outliers.index, columns = predictoras)\n",
                "\n",
                "X_test_SIN_outliers_norm = norm_SIN_outliers.transform(X_test_SIN_outliers)\n",
                "X_test_SIN_outliers_norm = pd.DataFrame(X_test_SIN_outliers_norm, index = X_test_SIN_outliers.index, columns = predictoras)\n",
                "\n",
                "\n",
                "## ESCALADO MIN-MAX (MINMAXIMIZACIÓN)\n",
                "### CON OUTLIERS\n",
                "scaler_CON_outliers = MinMaxScaler()\n",
                "scaler_CON_outliers.fit(X_train_CON_outliers)\n",
                "\n",
                "X_train_CON_outliers_scal = scaler_CON_outliers.transform(X_train_CON_outliers)\n",
                "X_train_CON_outliers_scal = pd.DataFrame(X_train_CON_outliers_scal, index = X_train_CON_outliers.index, columns = predictoras)\n",
                "\n",
                "X_test_CON_outliers_scal = scaler_CON_outliers.transform(X_test_CON_outliers)\n",
                "X_test_CON_outliers_scal = pd.DataFrame(X_test_CON_outliers_scal, index = X_test_CON_outliers.index, columns = predictoras)\n",
                "\n",
                "### SIN OUTLIERS\n",
                "scaler_SIN_outliers = MinMaxScaler()\n",
                "scaler_SIN_outliers.fit(X_train_SIN_outliers)\n",
                "\n",
                "X_train_SIN_outliers_scal = scaler_SIN_outliers.transform(X_train_SIN_outliers)\n",
                "X_train_SIN_outliers_scal = pd.DataFrame(X_train_SIN_outliers_scal, index = X_train_SIN_outliers.index, columns = predictoras)\n",
                "\n",
                "X_test_SIN_outliers_scal = scaler_SIN_outliers.transform(X_test_SIN_outliers)\n",
                "X_test_SIN_outliers_scal = pd.DataFrame(X_test_SIN_outliers_scal, index = X_test_SIN_outliers.index, columns = predictoras)\n",
                "\n",
                "# X_train_CON_outliers\n",
                "# X_train_CON_outliers_norm\n",
                "# X_train_CON_outliers_scal\n",
                "# X_train_SIN_outliers\n",
                "# X_train_SIN_outliers_norm\n",
                "# X_train_SIN_outliers_scal\n",
                "\n",
                "# X_test_CON_outliers\n",
                "# X_test_CON_outliers_norm\n",
                "# X_test_CON_outliers_scal\n",
                "# X_test_SIN_outliers\n",
                "# X_test_SIN_outliers_norm\n",
                "# X_test_SIN_outliers_scal"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DATASETS QUE HE IDO ACUMULANDO EN LOS PASOS 4 Y 5\n",
                "X_train_CON_outliers.to_excel(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/data/processed/X_train_CON_outliers.xlsx\", index = False)\n",
                "X_train_CON_outliers_norm.to_excel(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/data/processed/X_train_CON_outliers_norm.xlsx\", index = False)\n",
                "X_train_CON_outliers_scal.to_excel(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/data/processed/X_train_CON_outliers_scal.xlsx\", index = False)\n",
                "X_train_SIN_outliers.to_excel(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/data/processed/X_train_SIN_outliers.xlsx\", index = False)\n",
                "X_train_SIN_outliers_norm.to_excel(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/data/processed/X_train_SIN_outliers_norm.xlsx\", index = False)\n",
                "X_train_SIN_outliers_scal.to_excel(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/data/processed/X_train_SIN_outliers_scal.xlsx\", index = False)\n",
                "\n",
                "X_test_CON_outliers.to_excel(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/data/processed/X_test_CON_outliers.xlsx\", index = False)\n",
                "X_test_CON_outliers_norm.to_excel(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/data/processed/X_test_CON_outliers_norm.xlsx\", index = False)\n",
                "X_test_CON_outliers_scal.to_excel(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/data/processed/X_test_CON_outliers_scal.xlsx\", index = False)\n",
                "X_test_SIN_outliers.to_excel(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/data/processed/X_test_SIN_outliers.xlsx\", index = False)\n",
                "X_test_SIN_outliers_norm.to_excel(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/data/processed/X_test_SIN_outliers_norm.xlsx\", index = False)\n",
                "X_test_SIN_outliers_scal.to_excel(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/data/processed/X_test_SIN_outliers_scal.xlsx\", index = False)\n",
                "\n",
                "y_train.to_excel(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/data/processed/y_train.xlsx\", index = False)\n",
                "y_test.to_excel(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/data/processed/y_test.xlsx\", index = False)\n",
                "\n",
                "# SCALERS\n",
                "\n",
                "\n",
                "with open(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/models/norm_CON_outliers.pkl\", \"wb\") as file:\n",
                "  pickle.dump(norm_CON_outliers, file)\n",
                "with open(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/models/norm_SIN_outliers.pkl\", \"wb\") as file:\n",
                "  pickle.dump(norm_SIN_outliers, file)\n",
                "with open(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/models/scaler_CON_outliers.pkl\", \"wb\") as file:\n",
                "  pickle.dump(scaler_CON_outliers, file)\n",
                "with open(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/models/scaler_SIN_outliers.pkl\", \"wb\") as file:\n",
                "  pickle.dump(scaler_SIN_outliers, file)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Paso 6: Selección de características\n",
                "\n",
                "La **selección de características** (*feature selection*) es un proceso que implica seleccionar las características (variables) más relevantes de nuestro conjunto de datos para usarlas en la construcción de un modelo de Machine Learning, desechando el resto.\n",
                "\n",
                "Existen varias razones para incluirlo en nuestro análisis exploratorio:\n",
                "\n",
                "1. Simplificar el modelo para que sea más fácil de entender e interpretar.\n",
                "2. Reducir el tiempo de entrenamiento del modelo.\n",
                "3. Evitar el sobre ajuste al reducir la dimensionalidad del modelo y minimizar el ruido y las correlaciones innecesarias.\n",
                "4. Mejorar el rendimiento del modelo al eliminar las características irrelevantes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "datasets = [\n",
                "    X_train_CON_outliers,\n",
                "    X_train_CON_outliers_norm,\n",
                "    X_train_CON_outliers_scal,\n",
                "    X_train_SIN_outliers,\n",
                "    X_train_SIN_outliers_norm,\n",
                "    X_train_SIN_outliers_scal\n",
                "]\n",
                "dataset_names = [\n",
                "    \"X_train_CON_outliers\",\n",
                "    \"X_train_CON_outliers_norm\",\n",
                "    \"X_train_CON_outliers_scal\",\n",
                "    \"X_train_SIN_outliers\",\n",
                "    \"X_train_SIN_outliers_norm\",\n",
                "    \"X_train_SIN_outliers_scal\"]\n",
                "\n",
                "models = []\n",
                "accs = []\n",
                "for i,dataset in enumerate(datasets):\n",
                "  model = LogisticRegression(random_state=10) # Regresión Logística\n",
                "  model.fit(dataset, y_train) # Entreno el modelo\n",
                "  y_pred = model.predict(dataset)\n",
                "  acc = accuracy_score(y_train, y_pred)\n",
                "  accs.append(acc)\n",
                "  models.append(model)\n",
                "  print(f\"Accuracy del dataset {i}: {acc:.4f}\")\n",
                "\n",
                "best_acc = max(accs)\n",
                "best_index = accs.index(best_acc)\n",
                "print(f\"\\nEl mejor dataset es el dataset {best_index}, es decir, {dataset_names[best_index]} con accuracy = {best_acc:.4f}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train = X_train_CON_outliers_scal.copy()\n",
                "X_test = X_test_CON_outliers_scal.copy()\n",
                "\n",
                "selection_model = SelectKBest(f_classif,k = 5)\n",
                "selection_model.fit(X_train, y_train)\n",
                "\n",
                "ix = selection_model.get_support()\n",
                "X_train_sel_CON_outliers = pd.DataFrame(selection_model.transform(X_train), columns = X_train.columns.values[ix])\n",
                "X_test_sel_CON_outliers = pd.DataFrame(selection_model.transform(X_test), columns = X_test.columns.values[ix])\n",
                "X_train_sel_CON_outliers[target] = list(y_train)\n",
                "X_test_sel_CON_outliers[target] = list(y_test)\n",
                "X_train_sel_CON_outliers.to_csv(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/data/processed/X_train_sel_CON_outliers_clean.csv\", index = False)\n",
                "X_test_sel_CON_outliers.to_csv(\"/workspaces/machine-learning-python-template5-JohnnyXavierReyesBorbor/data/processed/X_test_sel_CON_outliers_clean.csv\", index = False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
